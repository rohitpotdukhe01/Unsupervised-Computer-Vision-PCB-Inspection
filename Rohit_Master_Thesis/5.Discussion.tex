\chapter{Discussion}

In this section, we will provide more deeper understanding of the results. Particularly, we focus on comparing the performance of supervised and unsupervised approaches, also PatchCore's performance relative to the baseline \gls{yolo} model, and to try and examine the reasons behind underperformance of newer models such as EfficientAD when compared to PatchCore. Also, we will investigate the consistently high recall value observed in most of the anomaly detection models in our experiments.

\section*{Supervised vs. Unsupervised Learning}

One of the key objective of this thesis was to evaluate the performance differences between the supervised and unsupervised models for anomaly detection. Supervised learning methods, in our case \gls{yolo} usually requires large amount of labeled training data. This can be challenging for the \gls{pcb} manufacturing, where defects can be rare and labeling requires domain experts, which can be time consuming and expensive. Whereas, unsupervised models such as PatchCore, \gls{dfm}, and EfficientAD uses patterns from normal data to identify anomalies, which reduces the dependency on labeled datasets. \textbf{In anomaly detection, unsupervised learning means that the model is trained only on normal data, allowing it to learn the features of normal data and flag any deviations from it as anomalies. Even though the model is only exposed to normal samples during training, it does not have access to labeled information about defects or a mask indicating their location, which is required during supervised training methods. This makes it unsupervised as the model is not explicitly taught where defects are but instead learns to detect anomalies from normal patterns on its own.} The results also demonstrates that despite having no information about defects location, it can still perform quite well, with PatchCore reaching an accuracy of 91.25\%, which is quite good when compared to the baseline \gls{yolo}v8-m with only 2.5\% difference in their accuracy. This results shows the effectiveness of unsupervised learning models, which can come close to performance of supervised models as well.

\section*{Performance of PatchCore vs Baseline \gls{yolo}}

The PatchCore model achieved an accuracy of 91.25\%, showing its effectiveness in anomaly detection. While \gls{yolo}v8-M and \gls{yolo}v8-L achieved higher overall accuracy scores of 93.75\% and 95\%, respectively, \gls{yolo} models showed better results in most aspects, particularly in achieving fewer false positives, as evident from their confusion matrices. \textbf{PatchCore, however, provided a competitive approach}, particularly in its unique patch-level analysis. Unlike \gls{yolo}, which processes entire image, PatchCore analyzes smaller patches of an image, which can be advantageous in certain scenarios. Despite this, the \gls{yolo} models generally outperformed PatchCore.

PatchCore's results are significant in addressing the 'cold start' problem, which refers to the lack of anomalous samples during training. By leveraging only normal data, \textbf{PatchCore provides an competitive alternative approach for industrial defect detection, successfully detecting anomalies without needing extensive labeled defect data like in supervised learning approaches}. This characteristic makes PatchCore suitable for quality assurance in high-volume production environments, where the ability to detect rare faults with minimal labeled data is crucial. Additionally, since no extensive manual labeling is required, this approach can help in significantly reducing costs.

%\section*{Why Newer Models couldn't Outperform PatchCore}
\section*{Limitations of Newer Models}

Although models like EfficientAD, FastFlow are more recent, with having specific strengths, such as computational efficiency and speed, they still couldn't surpass PatchCore's performance in terms of accuracy. There could be several reasons for this outcome. Firstly, PatchCore's dependency on memory banks of patch-level features allows for a rich representation of normal patterns, which facilitates for robust anomaly detection. The coreset subsampling method used in PatchCore reduces redundancy in the feature space, which helps improving it's efficiency when comparing new data with the stored nominal patches.

In contrast, even though EfficientAD being computationally efficient and fast due to its \gls{s-t} network structure, lacks the detailed patch-level comparison that PatchCore employs. EfficientAD focuses on lightweight architectures that excel in computational efficiency, but this can often come at the cost of the deep representational power needed to identify subtle anomalies in complex datasets. Similarly, \gls{dfm} relies on probabilistic modeling of deep features, but it lacks the fine granularity offered by PatchCore's patch-wise anomaly scoring, making it less effective when dealing with highly localized defects. FastFlow, which uses normalizing flows, also struggled to outperform PatchCore due to its limited ability to capture complex defect patterns. FastFlow's dependence on a more generalized feature extraction approach makes it less capable at identifying complex or subtle anomalies compared to PatchCore's specialized patch-level method.

In order to further highlight the differences in model performance, we compared key metrics such as accuracy, recall, and precision across across all the unsupervised models. PatchCore achieved an accuracy of 91.25\% indicating its strong ability to detect defects. \gls{yolo}v8-M and \gls{yolo}v8-L, while fast and achieving accuracies, showed better results in capturing anomalies with fewer false positives. EfficientAD, with its focus on computational efficiency, demonstrated quick inference times but could not match PatchCore's accuracy, primarily due to its lightweight architecture. \gls{dfm}, which uses probabilistic deep feature modeling, also fell short in accuracy compared to PatchCore, struggling with the complex and diverse nature of the defect patterns. This comparative analysis highlights the trade-offs between computational efficiency and detection accuracy among the different models.

\section*{High Recall in Anomaly Detection Models results}

A notable observation across the evaluated models was the consistent high recall, often reaching 1.0. This behaviour can be explained by the nature of anomaly detection tasks, in which the models are generally trained to identify any deviations from the learned distribution of normal data. In many industrial applications, it is crucial to capture all possible defects, even at the risk of producing false positives to ensure product reliability. This drives the models towards maximizing recall, ensuring that no defective sample is overlooked. High recall is particularly important in situations where the cost of missing a defect is far greater than the cost of examining a false positive.

However, this emphasis on recall may come with the drawback of lower precision, as observed in some models. Precision measures the ability to correctly classify anomalies without misclassifying normal samples, and a lower precision value indicates a higher rate of false positives. In practical terms, while a high recall ensures comprehensive defect coverage, the accompanying lower precision necessitates additional post inspection checks to filter out the false positives.

\section*{Final Thoughts}

While none of the unsupervised learning models could outperform the baseline \gls{yolo} model, but PatchCore still provided competitive results. Its ability to correctly detect anomalies without the need for labeled data or a mask during training makes it a valuable alternative, especially in settings where labeling is costly or impractical. This not only helps reduce the dependency on extensive manual labeling but also significantly cuts down the associated costs, making PatchCore a practical choice for industrial applications. 
