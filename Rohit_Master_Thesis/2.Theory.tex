\chapter{Theoretical Background}

\section{Related Works}

Image Classification has experienced significant growth over the last few decades, driven by increased computational capabilities, large amounts of available data, and the development of sophisticated algorithms. This section provides an overview of the major advancements in Image Classification, especially for its applications in industrial environments, such as electronics manufacturing. We will also explore the increasing significance of anomaly detection, emphasizing the transition from conventional supervised learning approaches to modern unsupervised learning methods.

\subsection{Evolution of Image Classification Techniques}

Image Classification is a critical task in the field of \gls{cv}, with numerous applications ranging from medical diagnostics to autonomous driving. Early image classification algorithms mainly relied on manual feature extraction, which involves domain experts designing these features, which could be used to differentiate between classes of images. These features were subsequently inputted into traditional machine learning algorithms like \glspl{svm} or \gls{k-nn} for classification \cite{LeCun2015}. Nevertheless, these techniques were limited by their dependence on handcrafted features, which could not effectively reflect intricate changes observed in real-world scenarios.

Subsequently, the introduction of \glspl{cnn} was a significant milestone in this field. They have introduced the concept of feature learning, whereby the network learns to extract relevant features from the raw image data using numerous layers of convolutional filters \cite{NIPS2012_c399862d}. This groundbreaking study in \cite{NIPS2012_c399862d} on AlexNet architecture delivered exceptional results on the ImageNet dataset for image classification. This success triggered various research into network architectures that are deeper and more complex, such as VGGNet \cite{simonyan2015deepconvolutionalnetworkslargescale}, GoogLeNet \cite{7298594}, and ResNet \cite{he2016deep}.

Among these, the work on the introduction of ResNet by \cite{he2016deep} was highly significant as it addressed the issues of vanishing gradients that tormented earlier deep networks \cite{simonyan2015deepconvolutionalnetworkslargescale} \cite{7298594}. ResNet made it possible to train much deeper networks by incorporating residual connections, resulting in substantial improvement in accuracy over various image classification benchmarks. Thus, \glspl{cnn} were firmly established as the leading approach for tasks like image classification.

\subsection{Applications of Image Classification in Industrial Settings}

In the industrial sector, special attention is given to image classification applications in quality control processes, particularly in automated visual inspection. Traditionally, the visual inspection process was carried out manually, where we had to rely on the experience of quality inspectors to ensure product quality. As a result of the varying levels of expertise among inspectors and the limitations of human abilities, this approach exhibits low efficiency, low accuracy, and inadequate real-time performance on a large-scale manufacturing process \cite{Gong_2020}.

With the rise of \gls{dl}, automated visual inspection systems are developed using \glspl{cnn} for real-time defect detection. One of the most notable models in industrial applications is \gls{yolo}, which is used for real-time object detection and has gained popularity in tasks requiring rapid image processing. The design of \gls{yolo} enables it to perform object detection in a single iteration over the network, rendering it highly efficient in cases where speed is critical. For example, during the PCB manufacturing process, \gls{yolo} can rapidly identify solder joints that do not meet quality standards. This offers a significant decrease in inspection times compared to traditional manual techniques \cite{redmon2016you}.

Although \gls{cnn}-based models such as \gls{yolo} have demonstrated their effectiveness in most industrial applications, they have certain limitations. One major obstacle is the requirement for a large labeled dataset to train such models properly. In many industrial applications, such as PCB inspection, defects are rare, and gathering labeled instances in large enough quantities for training purposes may become prohibitively expensive. In such instances, the labeling process is typically labor- and knowledge-intensive, resulting in high costs and time consumption\cite{FINK2020103678}.

\subsection{Anomaly Detection in Electronics Manufacturing}

In certain scenarios, due to the lack of labeled data, supervised learning methods are not feasible. As a result, there has been a growing interest in exploring unsupervised learning methods for anomaly detection. Anomaly detection refers to finding patterns in data that deviate from expected behavior. This makes it a suitable solution in industrial applications for identifying defects where normal instances are well represented, but anomalies are rare and diverse \cite{10.1145/1541880.1541882}.

In electronics manufacturing, anomaly detection is essential for tasks like solder joint inspection. Traditional techniques are based on probabilistic and distance-based models. These techniques are effective when dealing with simpler anomaly patterns but often face difficulties when dealing with complex and high-dimensional data, as explained by \cite{PIMENTEL2014215}. This article points to the limitation of these methods because they rely on predefined thresholds and basic data assumptions that fail to generalize on a wide variety of anomalies.

Further expanding on this foundation, recent advancements in \gls{dl} introduced more sophisticated methods, particularly by employing autoencoders\cite{bank2021autoencoders}. Neural networks, namely \glspl{vae}\cite{Kingma_2019}, provide substantial improvements in presenting complex data distributions. These autoencoders learn compression and reconstruction of inputs through training on normal operational data, thereby building a model of 'normality' that allows it to find those anomalous instances with higher reconstruction errors indicating a deviation from the normal\cite{bank2021autoencoders}. This approach improves the detection of complex patterns of defects to help maintain the integrity of products produced in the manufacturing processes. Other variants of autoencoders, such \gls{vae}, deep autoencoders, have been investigated for anomaly detection and have shown promising results \cite{Kingma_2019}.

Another increasingly popular method for anomaly detection is the \glspl{gan}\cite{goodfellow2014generativeadversarialnetworks}. \glspl{gan} consists of two networks, namely a generator and a discriminator, both of which are trained in parallel. The generator tries to produce data that closely resembles the real one, whereas the discriminator strives to differentiate between real and generated data. In anomaly detection, the generator is trained to generate normal instances, while the discriminator learns to identify deviations from this normal distribution as anomalies \cite{schlegl2017unsupervisedanomalydetectiongenerative}.

%\subsubsection{Gaps in Current Research}

%Although significant advancements have been made in image classification and anomaly detectionusing both supervised and unsupervised learning methods, there are still certain gaps in the current research. The most obvious obstacle is the effective integration of the two approaches that maximizes the advantages of each other. Although supervised models, such as \gls{yolo}\cite{redmon2016you}, succeed well with abundant labeled data, they struggle in applications where labels are limited or where anomalies have not been well-defined. While unsupervised models entail more flexibility and typically require less labeled data, many are incapable of pulling off the same level of precision achieved with supervision when it comes to fine-grained classification tasks \cite{9347460}.




%In recent years, the exponential growth in deep learning research has revolutionized the field of Computer Vision. Much of this change has been led by Convolutional Neural Networks (CNNs), making highly accurate image recognition tasks possible[1]. Since then, ResNet and EfficientNet have further moved this needle by resolving issues like vanishing gradients and model scaling optimization[2][3]. Moreover, Vision Transformers (ViTs) have provided a new perspective in image classification by using transformer architectures primarily tailored for natural language processing[4]. These innovations have extended the scope of various areas in which deep learning applications can be applied in different industries, enhancing the ability to analyze and interpret complex visual data.

%The application of image classification techniques in the electronics manufacturing industry has shown significant promise. Automated visual inspection systems utilizing CNNs can effectively identify and classify defects in various manufacturing processes, reducing the reliance on manual inspection and minimizing human error. For example, deep learning models have been successfully applied to detect defects in automotive parts, textile production, and food processing, demonstrating their versatility and effectiveness [5]. Moreover, the integration of these techniques has streamlined quality control processes, enabling faster and more accurate identification of defects, which is crucial for maintaining high standards of product quality and efficiency in production lines [6].

%Advances in anomaly detection, particularly through unsupervised learning, have also played a critical role in the manufacturing sector. Traditional methods often rely on labeled datasets, which can be time-consuming and costly to generate. Unsupervised anomaly detection techniques, such as autoencoders, variational autoencoders (VAEs), and generative adversarial networks (GANs), have alleviated this dependency by identifying deviations from normal patterns without the need for extensive labeling [7]. These methods have been applied to detect anomalies in a wide range of manufacturing contexts, from monitoring machinery health to ensuring the integrity of assembled products. By leveraging the inherent patterns in the data, unsupervised learning approaches have enabled more scalable and adaptable solutions for anomaly detection [8][9].

%The continuous evolution of these technologies underscores the transformative impact of deep learning on the manufacturing industry. By integrating advanced image classification and anomaly detection techniques, manufacturers can achieve more efficient and accurate inspection processes, ultimately leading to improved product quality and operational efficiency. This ongoing research and development highlight the potential for even greater advancements in the future, as deep learning models become more sophisticated and capable of addressing increasingly complex manufacturing challenges [10].

%References

%[1] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems 25 (2012).
%[2] He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
%[3] Tan, Mingxing, and Quoc V. Le. "EfficientNet: Rethinking model scaling for convolutional neural networks." International Conference on Machine Learning. PMLR, 2019.
%[4] Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).
%[5] Bhandarkar, Suchendra M., et al. "Deep learning based quality inspection in manufacturing." Procedia Manufacturing 26 (2018): 998-1006.
%[6] Zhang, Xiaolei, et al. "Deep learning-based quality inspection for manufacturing." IEEE Access 7 (2019): 61232-61245.
%[7] An, Jinwon, and Sungzoon Cho. "Variational autoencoder based anomaly detection using reconstruction probability." Special Lecture on IE 2.1 (2015).
%[8] Ruff, Lukas, et al. "Deep One-Class Classification." International Conference on Machine Learning. PMLR, 2018.
%[9] Kiran, B Ravi, Dilip Thomas, and Ranjith Parakkal. "An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos." Journal of Imaging 4.2 (2018): 36.
%[10] Bergmann, Paul, et al. "Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.

%\subsection{Machine Learning}

%Machine Learning is a subdomain of Artificial Intelligence that encompasses various techniques able to automatically discover patterns in data and use those patterns to predict future data[1].

\section{Supervised Image Processing}
\label{sec:supervised}

Supervised image processing is a subset of \gls{ml} where a model is trained using a labeled dataset. Labeled dataset means that each image in that dataset is tagged with its correct output label or category. With this approach, the model can learn to map inputs to specific outputs \cite{geeksforgeeks-sup-unsup}.

\subsection{Artificial Neural Networks (ANNs)}

\gls{ann} are computational processing systems inspired by how biological nervous systems like the human brain function. \gls{ann} mainly consists of numerous interconnected computational nodes, known as neurons, which are intertwined in a distributed fashion to collectively learn from the input and optimize the final output \cite{oshea2015introductionconvolutionalneuralnetworks}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{Images/ann_architecture.pdf}
    \caption{\gls{ann} is a three-layer \gls{fnn} made up of an input, a hidden, and an output layer \cite{oshea2015introductionconvolutionalneuralnetworks}.}
    \label{fig:ann architecture}
\end{figure}

Figure \ref{fig:ann architecture} shows the basic structure of a \gls{ann}. Input data will be loaded as a multidimensional vector into the input layer. Then, it will be distributed into the hidden layer. Then, hidden layers will make decisions based on the previous layer and evaluate how a stochastic change improves the final output, this process is known as learning. When multiple hidden layers are stacked next to each other, it is known as \gls{dl} \cite{oshea2015introductionconvolutionalneuralnetworks}.

\subsection{Convolutional Neural Networks (CNNs)}

\glspl{cnn}\cite{726791} is an extended version of \gls{ann}, which is primarily used for feature extraction from a grid-like matrix dataset \cite{GeeksforGeeks2024}. \glspl{cnn} is similar to traditional \gls{ann} as they consist of neurons that self-optimize through learning. Each neuron will receive input and perform operations like scalar product followed by a non-linear function, which is the basis of many \gls{ann}. The only significant difference between \glspl{cnn} and traditional \gls{ann} is that \glspl{cnn} are mainly used on images in the field of pattern recognition. This enables the encoding of image-specific features into the architecture, making it more suitable for image-focused tasks while reducing the parameters required for model configuration \cite{oshea2015introductionconvolutionalneuralnetworks}.

The term \glspl{cnn} indicates that the network uses a mathematical operation called \textbf{convolution}. \glspl{cnn} are neural networks that, in place of general matrix multiplication, use convolution in at least one of the layers \cite{Goodfellow-et-al-2016}.

\subsubsection*{\gls{cnn} Architecture :}

Basic \gls{cnn} architecture consists of three types of layers they are \textbf{convolutional layers}, \textbf{pooling layers}, and \textbf{fully connected layers}. A \gls{cnn} architecture is formed when these layers are stacked together. Figure \ref{fig:cnn architecture} shows \gls{cnn} architecture for MNIST\cite{6296535} classification.


\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{Rohit_Master_Thesis//Images/cnn_architecture.pdf}
    \caption{A common \gls{cnn} architecture \cite{oshea2015introductionconvolutionalneuralnetworks}}
    \label{fig:cnn architecture}
\end{figure}

The functionality of \gls{cnn} architecture shown in the above figure can be broken down into four key areas.

1. Similar to other types of \gls{ann}, the input layer holds the images pixel values \cite{oshea2015introductionconvolutionalneuralnetworks}.

2. The convolutional layer will determine the output of neurons linked to local input regions by calculating the scalar product of their weights and the corresponding input volume region. The \gls{relu} aims to apply an 'elementwise' activation function like sigmoid to the output generated by the preceding layer's activation \cite{oshea2015introductionconvolutionalneuralnetworks}.

3. The pooling layer will perform downsampling along the spatial dimensions of the input. This further reduces the number of parameters in the activation \cite{oshea2015introductionconvolutionalneuralnetworks}.

4. The fully connected layers will then perform the same functions as that in a standard \gls{ann}, aiming to produce class scores from the activations for classification purposes. To improve performance, it is also recommended to use \gls{relu} between these layers \cite{oshea2015introductionconvolutionalneuralnetworks}. 

By using this simple transformation technique, \glspl{cnn} can transform the original input layer by layer by using convolutional and downsampling techniques, producing class scores for classification and regression purposes \cite{oshea2015introductionconvolutionalneuralnetworks}. Let's look at the main components of the \gls{cnn} architecture in detail.

\subsubsection*{Convolutional Layer :}

The convolutional layer plays an essential role in the \glspl{cnn} functionality. The layer's parameters concentrate on using learnable \textbf{kernels}. These kernels are the set of learnable parameters.

These kernels are usually small in spatial dimensions but spread entirely along the input depth. Upon the data entering a convolutional layer, it convolves each filter across the spatial dimensions of the input to generate a 2D activation map as shown in figure \ref{fig:convolutional layer} \cite{oshea2015introductionconvolutionalneuralnetworks}.

%\begin{figure}
%    \centering
%    \includegraphics[width=1.1\linewidth]{Rohit_Master_Thesis//Images/conv_layer.pdf}
%    \caption{A visual representation of a covolutional layer. The centre element of the kernel is positioned over the input vector, from which then a weighted sum of itself and any nearby pixels is calculated and replaced \cite{oshea2015introductionconvolutionalneuralnetworks}.}
%    \label{fig:convolutional layer}
%\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{Rohit_Master_Thesis//Images/conv_layer_v2.png}
    \caption{The convolution operation involves the sliding of a convolution kernel(filter) over the input vector, where the kernel is multiplied by the pixel value at the corresponding positions of the input, and summing them to produce a feature map \cite{Zhao2024}.}
    \label{fig:convolutional layer}
\end{figure}

As the kernel glides through the input, it calculates the scalar product for each value in that kernel. Through this, the network will learn kernels that activate upon they detect a specific feature at a given spatial position of the input. These are commonly referred to as activations. Each kernel will have a corresponding activation map, stacked along the depth dimension to form a complete output volume from the convolutional layer \cite{oshea2015introductionconvolutionalneuralnetworks}.

To mitigate the problem of \gls{ann} where the models get too big to train effectively due to the full-connected nature of standard \gls{ann} neurons, each neuron in a convolutional layer is connected only to a small region of the input, known as \textbf{receptive field}. The connectivity depth is almost always equal to the input depth \cite{oshea2015introductionconvolutionalneuralnetworks}. To understand this, let's consider an example: if the network receives an input image measuring $64\times64\times3$(representing an RGB image), with the receptive field being of the size $6\times6$, each neuron will have a total of 108 weights within the convolutional layer($6\times6\times3$, with 3 representing the amount of connectivity across the depth of the volume). Whereas a standard neuron in other forms of \gls{ann} would have $12,288$ weights each \cite{oshea2015introductionconvolutionalneuralnetworks}.

Optimizing its output convolutional layers can also significantly reduce the complexity of the model. Three hyperparameters—depth, stride, and setting zero-padding—optimize these layers.

The \textbf{depth} of the output volume can be set manually using the number of neurons within the convolutional layer corresponding to the same region in the input. Although reducing this hyperparameter can significantly reduce the total number of neurons of the network, it can also significantly reduce the model's pattern recognition capabilities \cite{oshea2015introductionconvolutionalneuralnetworks}.


\textbf{Stride} is the number of rows and columns the receptive field will move across the input's spatial dimension \cite{Zhao2024}. For example, with stride set to 1, then the significantly overlapping receptive field will produce extensive activations. At the same time, a larger value of stride will reduce overlapping and produce an output of lower spatial dimensions \cite{oshea2015introductionconvolutionalneuralnetworks}.

\textbf{Zero-Padding} is a simple process of padding the border of the input with zeros. It is an effective way to enhance control of the dimensionality of the output volumes \cite{oshea2015introductionconvolutionalneuralnetworks}.

We can calculate the spatial dimensionality of convolutional layer output by using the following formula: 

\[
    \frac{(V - R) + 2Z}{S + 1}
\]

Here, V denotes the input volume size(height$\times$width$\times$depth), R denotes the receptive field size, Z denotes the zero-padding set, and S denotes the stride. If the result from the above formula is not a whole integer, then the stride has been set incorrectly, and the neurons cannot fit across the given input \cite{oshea2015introductionconvolutionalneuralnetworks}.

Despite these optimizations, models can still be huge using high-dimensional input like images. To further reduce the number of parameters significantly within the convolutional layer, \textbf{parameter sharing} is used. It works on the assumption that if a feature from one spatial region is useful for computation, then it is likely to be useful in another region as well \cite{oshea2015introductionconvolutionalneuralnetworks}.

\subsubsection*{Pooling layer :}

The pooling layers help reduce the dimensionality of the feature representation, thus reducing the number of parameters and computational complexity of the model. The pooling layer operates on each activation map in the input and reduces its dimensionality using the 'Max' function. \textbf{Max-pooling layer} is one of the most common types of pooling used in \glspl{cnn}. It selects the maximum activity value of all neurons to represent the particular region and extracts the most essential feature from the input feature map \cite{Zhao2024}. A $2\times2$ kernel is applied with a stride of 2 across the spatial dimensions of the input, reducing the activation map by $75\%$ of the original size while preserving the depth volume of the feature map \cite{oshea2015introductionconvolutionalneuralnetworks}. 

\textbf{Average pooling} computes the arithmetic mean of all the elements within the region, resulting in the mean value of the local response from the extracted feature mapping \cite{Zhao2024}. Figure \ref{fig:max pooling} shows the max pooling operation where the maximum value is used, and figure \ref{fig:average pooling} shows the average pooling operation, which performs the average operation to get the value.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Rohit_Master_Thesis//Images/max_pooling.png}
    \caption{Max pooling \cite{Zhao2024}}
    \label{fig:max pooling}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Rohit_Master_Thesis//Images/average_pooling.png}
    \caption{Average pooling \cite{Zhao2024}}
    \label{fig:average pooling}
\end{figure}

\subsubsection*{Fully-Connected Layer :}

The fully connected layer contains neurons directly connected to the neurons in the two neighboring layers, with no connections to any neurons within itself. It is similar to the way neurons are arranged in traditional \gls{ann} as can be seen in the figure \ref{fig:ann architecture} \cite{oshea2015introductionconvolutionalneuralnetworks}.

%\subsubsection*{Activation Function :}

%Activation functions are essential to neural networks, strengthening the network's representational and learning capabilities by learning the abstract features through nonlinear transformation \cite{dubey2022activationfunctionsdeeplearning}. With activation functions, the neural network can approximate any nonlinear function, making it applicable to a wide range of nonlinear models \cite{Zhao2024}. Some of the activation functions common properties are:

%1. It should add the nonlinear curvature in the optimization landscape to enhance training convergence of the network;

%2. It should not significantly increase the computational complexity of the model;

%3. During training, it should not hamper the gradient flow;

%4. It should preserve the data distribution and facilitate better network training \cite{dubey2022activationfunctionsdeeplearning}.

%Here, we will look at the most common and widely used activation functions, including Sigmoid, Tanh, Softmax, \gls{relu}, and Leaky \gls{relu}.

%\textbf{1. Sigmoid Activation Function :}

%The Sigmoid function, or the logistic function, ranges from 0 to 1. As seen from the figure \ref{fig:sigmoid tanh function curve}, sigmoid can be used to normalize the output and probability-based predictions \cite{Zhao2024}. The following is the sigmoid functions mathematical formula :

%\[ 
%    f(x) = \frac{1}{1 + e^{-x}}
%\]

%Figure \ref{fig:sigmoid tanh function curve} shows that the sigmoid gradient is smooth, preventing output values from jumping. Nevertheless, Sigmoid has many problems; for example, the chances of the vanishing gradient are high when the activation is near 0 or 1. There is also slow gradient descent convergence due to non-zero-centered output. Finally, due to the sigmoid function's exponential operation, the computation time of the model increases as well \cite{Zhao2024}.


%\begin{figure}[ht!]
%    \centering
 %   \includegraphics[width=0.8\linewidth]{Rohit_Master_Thesis//Images/sigmoid_tanh_af.png}
  %  \caption{Sigmoid and Tanh activation function curve \cite{Zhao2024}}
   % \label{fig:sigmoid tanh function curve}
%\end{figure}

%\textbf{2. Tanh Activation Function :}

%The hyperbolic tangent activation function(HTAF), or tanh, compresses the input vector in the range of -1 to 1 and offers a zero-centered output. The below formula and the figure \ref{fig:sigmoid tanh function curve} shows the tanh curve and mathematical representation:

%\[
%    f(x) = \frac{2}{1 + e^{-2x}} - 1
%\]

%From the figure \ref{fig:sigmoid tanh function curve}, we can see that tanh and sigmoid function are relatively similar S-shaped curves. Tanh and Sigmoid have the following relationship:

%\[
%    Tanh(x) = 2Sigmoid(2x) - 1
%\]

%In practice, tanh is used more than sigmoid as it solves the sigmoid functions problem of not centering the output to zero. However, like sigmoid, tanh also suffers from vanishing gradient problems for extreme input values \cite{Zhao2024}.

%\textbf{3. Softmax Activation Function :}

%A softmax activation function is used in multi-class classification problems. It compresses the input vectors into probabilities from 0 to 1, all of which sum up to 1. In the K classification task, the generated probabilities can represent each category, with the larger value indicating the higher probability that it belongs to that particular category \cite{Zhao2024}. Figure \ref{fig:softmax function curve} shows the softmax function curve, and the below formula shows how softmax is formulated mathematically:

%\[
%    f(x) = \frac{e^{x_i}}{\sum_{j=1}^{k} e^{x_j}}
%\]

%\begin{figure}[ht!]
%    \centering
%    \includegraphics[width=0.7\linewidth]{Rohit_Master_Thesis//Images/softmax.png}
 %   \caption{Softmax activation function curve \cite{Zhao2024}}
  %  \label{fig:softmax function curve}
%\end{figure}

%The gradient becomes zero when the softmax function encounters a negative input value. Therefore, the weights for activation in that region will not update throughout backpropagation, resulting in a dead neuron that never activated \cite{Zhao2024}.

%\textbf{4. ReLU Activation Function :}

%ReLU, or Rectified Linear Unit, is a segmented linear function, as shown in figure \ref{fig:relu function curve}. It is a fast, simple activation function which is essentially a ramp function given by the following formula:

%\[
%    f(x) = max(0,x)
%\]


%\begin{figure}[ht!]
 %   \centering
  %  \includegraphics[width=0.8\linewidth]{Rohit_Master_Thesis//Images/relu_af.png}
   % \caption{ReLU function curve \cite{Zhao2024}}
    %\label{fig:relu function curve}
%\end{figure}

%For positive input, the derivative is 1, which improves the vanishing gradient problem and speeds up the gradient descent convergence. It is also faster than the sigmoid and tanh functions because the ReLU function only has linear relationships. However, this function suffers from a dying ReLU problem, which is when the input is negative, the gradient will be precisely zero, and the neurons will most likely die during training \cite{Zhao2024}.

%\textbf{5. Leaky ReLU Activation Function :}

%Leaky ReLU addresses the dying ReLU problem to some extent by introducing very small linear components for negative inputs to solve zero gradients associated with negatives and extending the range of ReLU. Although Leaky ReLU has all the features of ReLU, in practice, it is not always the case that Leaky ReLU is better than ReLU \cite{Zhao2024}. Below is the mathematical formulation of Leaky ReLU, and figure \ref{fig:leaky relu function curve} shows its function curve.

%\[
%f(x) =
%\begin{cases} 
%    x, & \text{if } x \geq 0 \\
%    ax, & \text{if } x < 0 
%\end{cases}
%\]

%\begin{figure}[ht!]
%    \centering
%    \includegraphics[width=0.8\linewidth]{Rohit_Master_Thesis//Images/leaky_relu_af.png}
%    \caption{Leaky ReLU function curve \cite{Zhao2024}}
%    \label{fig:leaky relu function curve}
%\end{figure}

\subsection{ResNet}
\label{subsec:ResNet}

Deep Convolutional neural networks have led to numerous advancements in image classification. The network's depth plays a crucial role in this achievement, but deeper networks still face a critical problem of \textbf{degradation}. The degradation problem occurs when deeper neural networks start to converge: with increasing network depth, the accuracy gets saturated and then rapidly declines. This is caused by an increase in number of layers, which results in higher training errors. To address this degradation problem, a deep residual learning framework was proposed. In this framework, instead of relying on each stacked layer to directly match a specific underlying mapping, the layers are designed to fit a residual mapping \cite{he2016deep}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.2\linewidth]{Rohit_Master_Thesis//Images/residual_block.pdf}
    \caption{A residual block \cite{he2016deep}}
    \label{fig:residual block}
\end{figure}

\subsubsection*{Residual Learning :}

As mentioned above, instead of expecting stacked layers to approximate mapping function H(x), where x denotes the inputs to the first of this stacked layer. The layers are explicitly let to approximate a residual function $F(x) := H(x) - x$. The original function then becomes $F(x) + x$. This makes the learning process much easier, although it is expected that both forms can asymptotically approximate the desired functions \cite{he2016deep}.

The above mentioned residual learning is applied to every few stacked layers. The structure of a residual block is shown in the figure \ref{fig:residual block}. The basic building block can be defined for this approach as:

$ y = F(x, \left\{ W_{i}\right\}) + x$.

Here, the input and output vectors are represented by x and y of the considered layers; the function $F(x, \left\{ W_{i}\right\})$  is the residual mapping that needs to be learned. The operation $F + x$ is carried out by a \textbf{shortcut connection}, also known as skip connection, and element-wise addition as shown in figure \ref{fig:residual block}. This shortcut connection does not introduce any extra parameters or computational complexity except for the minor element-wise addition. This ensures a fair comparison between plain and residual networks, which have the same number of parameters, width, depth, and computational cost \cite{he2016deep}.

\subsubsection*{Network Architecture :}

\clearpage
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Rohit_Master_Thesis//Images/resnet_arch.pdf}
    \caption{Network architecture example: Left: VGG-19 model serves as a reference. Middle: a plain network consisting of 34 parameter layers. Right: a residual network with 34 parameter layers \cite{he2016deep}.}
    \label{fig:resnet architecture}
\end{figure}

Two models are described in the figure \ref{fig:resnet architecture}. The plain baseline network(figure \ref{fig:resnet architecture}, middle) draws inspiration from the philosophy of VGG nets\cite{simonyan2015deepconvolutionalnetworkslargescale} (Figure \ref{fig:resnet architecture}, left), which mainly uses $3\times3$ filters. It maintains the time complexity per layer by doubling the number of filters when the feature map size is halved. Downsampling is performed by convolutional layer with stride 2. The network finishes with a global average pooling layer followed by a 1000-way fully connected layer with softmax \cite{he2016deep}.

Short connections are added based on the plain network, which transforms the network into its counterpart residual version. Identity shortcuts match input-output dimensions(solid line shortcuts in figure \ref{fig:resnet architecture}). For when dimensions increase (dotted line shortcuts in figure \ref{fig:resnet architecture}), either the shortcut performs identity mapping, with zero-padding, or projection shortcuts($1\times1$ convolutions) are used \cite{he2016deep}.

\subsubsection*{Performance :}

The 18-layer and 34-layer \gls{resnet} show that the deeper 34-layer \gls{resnet} performs better than the 18-layer \gls{resnet} by 2.8\%. Also, the 34-layer \gls{resnet} significantly reduces training error and shows good generalization to the validation data. This shows that the degradation problem is well tackled in this setting, leading to accuracy gains from increased depth. Compared to the plain network(figure \ref{fig:resnet architecture}, middle), the 34-layer \gls{resnet} reduced the top-1 error by $3.5\%$; this verifies the efficacy of residual learning on very deep networks. It was also observed that 18-layer plain networks are more accurate, but the 18-layer \gls{resnet} converges faster \cite{he2016deep}.

\subsubsection*{Deeper Bottleneck Architectures :}

To scale the networks to 50, 101, and 152 layers, a bottleneck design is used, where the building blocks are modified as bottleneck layers, in which the residual function is stacked with three layers($1\times1$, $3\times3$, $1\times1$ convolutions). The $1\times1$ are responsible for dimensions reduction filled by restoration, which creates a bottleneck in the $3\times3$ layer with smaller input and output dimensions \cite{he2016deep}.

\textbf{50-layer \gls{resnet}:} In this architecture, each 2-layer block in a 34-layer network is replaced with a 3-layered bottleneck block, which results in a 50-layer \gls{resnet} \cite{he2016deep}.

\textbf{101-layer \& 152-layer \glspl{resnet}:} 3-layer blocks are used for the construction of 101-layer and 152-layer \glspl{resnet}. Even when the depth significantly increases, the 152-layer \gls{resnet} still has lower complexity than VGG-16/19 networks \cite{he2016deep}.

The 50/101/152-layer \glspl{resnet} show significantly higher accuracy compared to the 34-layer versions. No degradation problem is observed, due to which high accuracy can be achieved \cite{he2016deep}.

\subsection{Vision Transformer}

Transformers introduced by \cite{vaswani2017attention} has become the go-to model in \gls{nlp} related tasks. Transformer's computational efficiency and scalability allow for exceptional model training with over 100B parameters. However, when it comes to \gls{cv}, convolutional architectures like ResNet\cite{he2016deep} are still dominant. Therefore, inspired by the success of transformers in \gls{nlp}, the standard transformer was directly applied to images with minimal modifications. For this, images were split into patches, and then the transformer was given the linear embeddings of these patches as input. These image patches are treated the same way as a token(words) as in \gls{nlp} tasks, and the training is carried out in a supervised fashion \cite{dosovitskiy2020image}.

\subsubsection*{Architecture :}

Figure \ref{fig:vit architecture} shows an overview of the model's architecture. The standard transformer takes a 1D sequence of token embeddings as input. To process 2D images, firstly, the image is reshaped into flattened 2D patches called the patch embeddings \cite{dosovitskiy2020image}. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{Rohit_Master_Thesis//Images/vit_architecture.pdf}
    \caption{This is the proposed \gls{vit} architecture. Here, an image is split into fixed-sized patches, each of them is then linearly embedded, with that positional embedding is added, and then the resulting sequence is given as input to a standard transformer encoder. A "classification token" is added to the sequence to perform classification \cite{dosovitskiy2020image}.}
    \label{fig:vit architecture}
\end{figure}

Similar to BERT's [class] token, a learnable classification token is prepended to the sequence of the patch embeddings. Its output from the transformer encoder is used as image representation for classification. During both training and fine-tuning, a classification head is attached to this output, which consists of a \gls{mlp} with one hidden layer during pre-training and a single linear layer at fine-tuning time. To keep the positional information, positional embeddings are added along with the patch embeddings. The transformer encoder, as seen on the right side of the figure \ref{fig:vit architecture}, has alternating layers of \gls{msa} and \gls{mlp} blocks. Each block also includes \gls{ln}, and residual connections, these are applied before each block and after each block respectively. The \gls{mlp} consists of two layers with a GELU activation function. These layers process the sequence of patch embeddings and the [class] token \cite{dosovitskiy2020image}.

\subsection{Data-efficient image Transformers (DeiT)}

\gls{deit} was introduced in the paper \cite{pmlr-v139-touvron21a}, which, unlike \gls{vit}\cite{dosovitskiy2020image} does not need large-scale training and uses only Imagenet as a training set, and also need fewer computational resources. \gls{deit} introduced a token-based distillation strategy to distill information into the model \cite{pmlr-v139-touvron21a}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\linewidth]{Rohit_Master_Thesis//Images/deit_distillation_token.pdf}
    \caption{\gls{deit} distillation procedure\cite{pmlr-v139-touvron21a}.}
    \label{fig:deit distillation token}
\end{figure}

\subsubsection*{Distillation strategy :}

For a teacher model, a strong image classifier is used, it can be either a \gls{cnn} or a mixture of classifiers. One key innovation in \gls{deit} is the introduction of a "distillation token". This procedure is shown in figure \ref{fig:deit distillation token}. This distillation token is used similarly to the class token, which is added to the initial embeddings. It interacts with other tokens using the self-attention. The distillation token embedding is output by the network after the last layer and optimized by using the distillation component of the loss. This distillation embedding helps the model learn from the teacher's output, which remains complimentary to the class embedding \cite{pmlr-v139-touvron21a}.

\subsection{Class-Attention in Image Transformers (CaiT)}

\gls{cait} is a vision transformer type. It makes two main modifications to the original \gls{vit} architecture. Firstly, a new layer scaling method termed LayerScale is used, which adds a learnable diagonal matrix at the output of each residual block. This matrix is initialized close to 0, which improves the training dynamics. Secondly, class-attention layers are also introduced. This creates an architecture in which the transformer layers, which involve self-attention between the patches, are separated from the class-attention layers, which are devoted to extracting the content of the processed patches into a single vector, so it can be linear classifiers input \cite{touvron2021going}.

\clearpage
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.75\linewidth]{Rohit_Master_Thesis//Images/cait_architecture.pdf}
    \caption{In this figure, the original \gls{vit} is on the left, here the CLS token is inserted with patch embeddings. This is counterproductive, as the same weights are used for two purposes. In the middle architecture, the author shows that inserting the CLS token later on improves performance. Moreover, further in the \gls{cait} architecture(right), the patch embeddings are frozen when inserting a CLS token to save computational resources. So the last part of the network, usually the last two layers, is fully focused on summarizing the information that will be given as an input to the linear classifier \cite{touvron2021going}.}
    \label{fig:cait architecture}
\end{figure}

\subsubsection*{Architecture :}

The \gls{cait} consists of two processing stages, as seen in the figure \ref{fig:cait architecture}.

\paragraph{Self-Attention Stage :} In this stage, the CLS class token is inserted later in the transformer. This resolves the inconsistency of the first layers of the transformer, which are then entirely used for performing self-attention between patches only \cite{touvron2021going}. 

\paragraph{Class-Attention Stage :} A set of layers compiles the patch embeddings into a class embedding CLS that is then given to a linear classifier. Here, only the class embeddings are updated. However, the main difference is that, within \gls{cait} architecture, the information is not copied from class embedding to the patch embeddings during the forward pass.

\subsection{You Only Look Once (YOLO)}
\label{subsec:yolo}

The human visual system operates with amazing speed and accuracy, which helps us perform complex tasks like driving with minimal conscious thought. Similarly, fast and accurate algorithms for object detection could facilitate computers driving a car without the need for specialized sensors and assistive devices to provide real-time scene information to human users. This could pave the way for better responsive robotic systems. The object detection algorithms before \gls{yolo} used classifiers and repurposed them to detect objects by evaluating them across different locations and scaling them in a test image. These methods were relatively slow and complex to optimize \cite{redmon2016you}.

\gls{yolo} approaches object detection by treating it as a single regression problem, where it directly mapping image pixels to bounding box coordinates and class probabilities. That means you only look once (\gls{yolo}) at an image to predict and present what the object is and what its location is. As shown in the figure \ref{fig:yolo system}, a single \gls{cnn} can predict multiple bounding boxes and, at the same time, their class probabilities. It is trained on full images and focuses on optimizing detection performance directly \cite{redmon2016you}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{Rohit_Master_Thesis//Images/yolo_system.pdf}
    \caption{\gls{yolo} Detection System: \gls{yolo} makes the image processing very simple. It first resizes the image, then runs a single \gls{cnn} on an image, and then gives thresholding value to the detections based on the model's confidence \cite{redmon2016you}.}
    \label{fig:yolo system}
\end{figure}


\subsubsection*{ How \gls{yolo} works :}

As stated, \gls{yolo} unifies the separate object detection components into a single neural network. This network uses features from the complete image to predict all bounding boxes across all classes simultaneously. \gls{yolo} enables training from start to finish and achieves real-time speeds, all while ensuring a high level of average precision. \gls{yolo} divides the input image into an $S\times S$ grid. Every grid cell is responsible for detecting an object whose center falls into that grid cell. Every grid cell predicts bounding boxes and its confidence scores. This score reflects the model's confidence in the box containing an object and how accurate the predicted box is. If the object is detected, the confidence score should be equal to \gls{iou} between the ground truth and the predicted box; if no object is detected, the confidence score should be zero. At the test time, the conditional class probabilities and the box confidence predictions are multiplied to get the final detection as shown in the figure \ref{fig:yolo model} \cite{redmon2016you}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Rohit_Master_Thesis//Images/yolo_model.pdf}
    \caption{This figure shows the working of \gls{yolo} model \cite{redmon2016you}.}
    \label{fig:yolo model}
\end{figure}

\subsubsection*{Limitations of \gls{yolo}v1 :} 

\gls{yolo} imposes significant spatial constraints on bounding box predictions by allowing each grid cell to predict only one class and two bounding boxes. This reduces the model's ability to predict multiple nearby objects, like small objects that can appear in a group like flocks of birds \cite{redmon2016you}.

\gls{yolo} struggles when generalizing objects in new or unusual aspect ratios or configurations. Lastly, \gls{yolo}'s loss function does not differentiate between small and large bounding boxes. A small error in a large box does not have much impact, but a small error in a small box affects the \gls{iou}. The leading cause of this problem is the \gls{yolo}'s incorrect localization \cite{redmon2016you}.

\subsection*{\gls{yolo}'s versions: }

\textbf{\gls{yolo}v2} \cite{redmon2017yolo9000}, also known as YOLO9000. It integrated several existing techniques of that time. The entire object detection architecture was converted into a full convolutional network, which helps achieve high accuracy and speed. Later, the high-resolution and low-resolution features were combined, and an anchor-based prediction method was adopted. Due to its simple input and output formats, \gls{yolo}v2 remains one of the mainly used object detection methods in the maintenance and development of various industrial settings, particularly on low-end devices which has very limited computing capabilities \cite{wang2024yolov1}.

\textbf{\gls{yolo}v3} \cite{redmon2018yolov3}, integrated advanced technology of the existing object detection and made the necessary optimizations to one-stage object detectors. \gls{yolo}v3 has architecture which combines \gls{fpn}, allowing for simultaneous predictions across multiple scales. \gls{yolo}v3 made notable changes to the label assignment task. There are two changes in the \gls{yolo}v3. The first change involves assigning a ground truth to a single anchor, and the second change involves transitioning from soft label to hard label for \gls{iou}-aware objectness. \gls{yolo}v3 is still the most popular version of \gls{yolo} series \cite{wang2024yolov1}.

\textbf{Scaled-\gls{yolo}v4} \cite{wang2021scaled}, can be used for edge and clound computing both. Due to the efforts of the DarkNet and PyTorch \gls{yolo}v3 communities, scaled-\gls{yolo}v4 can forgo the pre-training steps necessary with ImageNet and instead directly use a train-from-scratch method to achieve high-quality object detection outcomes. Scaled-\gls{yolo}v4 has introduced CSPNet into \gls{pan}, significantly improving the speed, parameters, accuracy, and number of calculations. Scaled-\gls{yolo}v4 also introduced model scaling methods for different edge devices and offers three types of models: P5, P6, and P7. During training, it also used the decoder and label assignment strategy introduced in the initial version of \gls{yolo}v5 \cite{wang2024yolov1}.

\textbf{\gls{yolo}v5} \cite{ultralytics2024yolov5}, uses the design concepts of PyTorch \gls{yolo}v3 with a more simplified and revised architecture. It inherits many functions of \gls{yolo}v3, like auto anchor and hyperparameter search algorithms. The successive versions of \gls{yolo}v5 were optimized for speed-accuracy trade-off and activation functions. With continuous version updates and maintenance, \gls{yolo}v5 became one of the most popular \gls{yolo} development platforms. The two main features are Power-based Decoder and Neighborhood Positive Samples. \gls{yolo}v5 introduces a power-based decoder to enhance training stability, and to make up for the deficiency caused by recall, more neighbor grids are added as positive samples \cite{wang2024yolov1}.

\textbf{\gls{yolo}v6} \cite{ultralytics2024yolov6} is specially designed for industry applications, so a lot of effort was put into the quantization issues of earlier versions \cite{wang2024yolov1}. \gls{yolo}v6 introduced some unique features, which are the Bidirectional Concentration (BiC) module, which is integrated in the neck of the detector, improving the localization signals while achieving better performance with minimal impact on speed. The Anchor-Aided Training (AAT) strategy is introduced in the model that leverages the advantages of both the anchor-based and anchor-free paradigms, all while maintaining the inference efficiency. To enhance the smaller model's performance, a new self-distillation strategy is introduced, which focuses on improving the auxiliary regression branch during the training phase while also removing it during inference to prevent a significant decrease in speed \cite{ultralytics2024yolov6}.

\textbf{\gls{yolo}v7} \cite{ultralytics2024yolov7} outperformed other object detectors like Scaled-\gls{yolo}v4, \gls{yolo}v5, \gls{yolo}X, and many more in terms of speed and accuracy. It introduced a planned re-parameterized model, which serves as a strategy applicable to layers across different networks, with the concepts of gradient propagation path. To solve the issue of training a model with multiple output layers, that is, assigning dynamic targets for the outputs of different branches. A new label assignment technique called coarse-to-fine lead guided label assignment. \gls{yolo}v7 also introduced "extend" and "compound scaling" methods for real-time object detection, which can effectively use parameters and computation. Also, the methods proposed by \gls{yolo}v7 can significantly reduce about 40\% of parameters and about 50\% of computation and also have improved detection accuracy and faster inference speed \cite{ultralytics2024yolov6}.

\textbf{\gls{yolo}v8} \cite{ultralytics2024yolov8} is a refined iteration of \gls{yolo}v5 with overall updates to the way the API is used and many code optimizations. Changes in \gls{yolo}v7 ELAN architecture are made while incorporating an additional residual connection, while the decoder remains the same as that of \gls{yolo}v6 2.0. It effectively consolidates the APIs of various downstream tasks and links them in a sequential manner. The underlying code optimizations in \gls{yolo}v8 have resulted in a 30\% increase in training performance. \gls{yolo}v8 provides a simple API to connect with the detection models with various downstream tasks \cite{wang2024yolov1}. It also adopted an anchor-free split Ultralytics head, which improves accuracy and also contributes to a more efficient detection process in contrast to anchor-based approaches \cite{ultralytics2024yolov6}.

\section{Unsupervised Image Processing}
\label{sec:unsupervised image processing}

Unsupervised image processing is also part of \gls{ml}, but unlike the supervised learning we saw in section \ref{sec:supervised}, it does not require any labeled data. Instead, it focuses on clustering similar data points, discovering patterns and relationships, or detecting anomalies without explicitly telling it where the defect is \cite{geeksforgeeks-sup-unsup}. This approach is beneficial when labeled data is scarce or expensive to obtain.

Below, we will discuss and understand the unsupervised models we used in our experiments.

%\subsection{Anomaly Detection in Unsupervised Learning}

\subsection{PatchCore}
\label{subsec:patchcore}

PatchCore is a state-of-the-art approach developed for the efficient detection of anomalies in industrial settings, especially when there is a lack of defective samples or they are undefined. This approach has been specifically tailored to tackle the challenges of the cold-start problem in which models are exclusively trained on non-defective(nominal) images. PatchCore excels by utilizing a memory bank of nominal patch features and techniques like locally aware patch features and coreset subsampling, which are explained below \cite{roth2022totalrecallindustrialanomaly}. Figure \ref{fig:patchcore architecture} shows the overview of the PatchCore model.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.1\textwidth]{Rohit_Master_Thesis//Images/patchcore_architecture_figure.png}
    \caption{Overview of PatchCore: The nominal samples are decomposed into a memory bank of neighborhood-aware patch-level features. To minimize redundancy and the time required for inference, this memory bank is downsampled using greedy coreset subsampling. During the testing phase, images are classified as anomalies if any single patch is anomalous, and pixel-level anomaly segmentation is generated by assigning a score to each patch feature \cite{roth2022totalrecallindustrialanomaly}.}
    \label{fig:patchcore architecture}
\end{figure}

\subsubsection*{Locally Aware Patch Features :} 
The key advancement of PatchCore is the utilization of locally aware patch features. Contrary to traditional approaches that utilize global image features, PatchCore targets local patches of the image. It extracts the mid-level features that capture the contextual and spatial relationships within these patches. By maintaining local awareness, the model can preserve important details that might get lost when replying on the generalized global features \cite{roth2022totalrecallindustrialanomaly}.

The locally aware features are derived from the intermediate layers of a pre-trained \gls{cnn} \cite{726791}, namely WideResNet-50 \cite{zagoruyko2017wideresidualnetworks}. Focusing on mid-level features lets PatchCore avoid the pitfalls of over-generalization and inherent ImageNet class bias, which are common problems when relying on deeper, high-level features. The outcome is a more nuanced and contextually rich representation of the nominal data, which is essential for identifying subtle anomalies that could otherwise go unnoticed \cite{roth2022totalrecallindustrialanomaly}.

\subsubsection*{Memory Bank and Coreset Subsampling :}

PatchCores memory bank is built using these locally aware patch features, serving as a repository of nominal patch features, which are then used to compare against test images. However, handling a large memory bank in real-time industrial applications can be computationally expensive. In order to mitigate this issue, PatchCore uses coreset subsampling, a method that reduces the memory bank size by selecting the most representative features without compromising the model's performance \cite{roth2022totalrecallindustrialanomaly}.

Coreset subsampling employs a greedy selection algorithm, which ensures that the patches retained in the memory bank most accurately represent the overall distribution of the nominal data. The decrease in memory bank size is essential for achieving faster inference times, making PatchCore both accurate and highly efficient \cite{roth2022totalrecallindustrialanomaly}.

\subsubsection*{Anomaly Detection and Localization :}

During testing, PatchCore compares the patch features of a test image and the features stored in the memory bank. The metric Euclidean distance is used to compare each patch in the test image with the nearest patch in the memory bank. If any patch shows a significant deviation from the stored nominal patches, the image is flagged as anomalous. The anomaly score, which serves as a robust measure for anomaly detection, is determined by calculating the maximum distance observed across all patches \cite{roth2022totalrecallindustrialanomaly}.

PatchCore extends this approach to localize anomalies by creating a detailed segmentation map. A score is assigned to each patch in the test image depending on its proximity to the nearest nominal patch. These scores are subsequently mapped back to the original image, resulting in a localization map that precisely highlights the areas where the anomalies occur \cite{roth2022totalrecallindustrialanomaly}. This functionality is especially essential in industrial settings, where the ability to not only detect anomalies but also accurately determine their exact positions for quality control and remediation.

\subsubsection*{Performance and Applications :}

PatchCore has been rigorously tested across multiple benchmark datasets, such as MVTec AD dataset \cite{8954181}, where it delivered exceptional performance, achieving an \gls{auroc} of up to 99.6\%. This result significantly improves existing methods, effectively reducing detection error rates by half. Due to its ability to achieve high accuracy with minimal training data, PatchCore is especially well-suited for industrial settings where it can be challenging to gather a large number of defective samples \cite{roth2022totalrecallindustrialanomaly}.

\subsection{Deep Feature Modeling (DFM)}
\label{subsec:dfm}

\gls{dfm} is an efficient method for anomaly detection by utilizing \gls{dl} to extract feature representations and to model the distribution of normal data. This method excels in situations where the anomalies are rare and not well-defined, making it highly valuable for a range of industrial applications \cite{ahuja2019probabilisticmodelingdeepfeatures}.

The key concept of \gls{dfm} involves utilizing \gls{dnn} high-dimensional features from normal data and subsequently modeling the distribution of these features. The assumption is that the anomalies will significantly deviate from the learned distribution, facilitating accurate detection. \gls{dfm} leverages the ability of \gls{dl} to automatically learn complex feature representations from raw data, in contrast to traditional methods that rely on manually crafted features \cite{ahuja2019probabilisticmodelingdeepfeatures}.

\gls{dfm} is based on the notion that the normal data can be well represented in a lower dimensional feature space, while the anomalies stand out as outliers. By modeling the distribution of normal data features, \gls{dfm} can accurately identify points that deviate from the norm as anomalies \cite{ahuja2019probabilisticmodelingdeepfeatures}.

\subsubsection*{Feature Extraction and Representation :}

In \gls{dfm}, a \gls{dnn} most often a \gls{cnn} is used to extract features from the input data. These features are usually extracted from the intermediate layers of the network, which capture varying levels of abstraction, ranging from low-level edges and textures to more complex semantic information. The model's performance can be greatly influenced by the choice of network architecture and the specific layer from which features are extracted \cite{8954181}.

These features are then utilized to create a feature representation space. In this space, the normal data points are expected to cluster together, whereas anomalies should appear distant from these clusters. To enhance visualization and anomaly detection\cite{ahuja2019probabilisticmodelingdeepfeatures}, the dimensionality of this feature space is often reduced using methods like \gls{pca}\cite{Bishop2006} or \gls{t-sne}\cite{JMLR:v9:vandermaaten08a}.

\subsubsection*{Density Estimation and Anomaly Scoring :}

\gls{dfm} uses density estimation methods to model the distribution of normal data after constructing the feature representation space during the training process. Two commonly used methods for this purpose are \gls{gmm} or \gls{kde}, which estimate the probability density function of normal data. The idea here is that the areas in the feature space that have a high density of normal data, whereas regions with a low concentration indicate possible anomalies \cite{ahuja2019probabilisticmodelingdeepfeatures}.

A likelihood-based anomaly score is assigned to each new data point based on its deviation from the modeled distribution. Anomalies are identified by flagging points with low likelihood under the modeled distribution. This method is highly effective because it does not require labeled data for anomalies. It is well-suited for unsupervised learning situations where only normal data is accessible during training \cite{ahuja2019probabilisticmodelingdeepfeatures}.

\subsubsection*{Anomaly Detection and Evaluation :}

During testing, the trained model is given new data points to extract their feature representations, which are then evaluated against the density model to calculate an anomaly score. Again, anomalies are identified as points for which the model assigns low likelihood \cite{ahuja2019probabilisticmodelingdeepfeatures}.

\gls{dfm} has been extensively tested on several benchmark datasets, proving its effectiveness in detecting anomalies across various domains. The evaluation of the model's performance is done using metrics such as the \gls{auroc}, \gls{aupr}, and accuracy. \gls{dfm} demonstrated competitive results with \gls{auroc} score of about 98\%, outperforming traditional methods for anomaly detection \cite{ahuja2019probabilisticmodelingdeepfeatures}.

\subsection{Deep Feature Kernel Density Estimation (DFKDE)}
\label{subsec:dfkde}

\gls{dfkde} is a fast one-class anomaly detection model. It consists of two stages:

1. Feature extraction stage,

2. Anomaly detection stage,

In the feature extraction stage, the features are extracted using \gls{dnn} as the backbone, such as ResNet, which was pre-trained on ImageNet\cite{5206848} dataset \cite{10208786}. The penultimate layer, which is the average pooling layer of the backbone, is used to obtain a semantic feature vector of a fixed length of 2048 \cite{Anomalib2024}.

In the anomaly detection stage, once the features are extracted, then these features undergo dimensionality reduction using \gls{pca}\cite{IBM2023} to get the first 16 principal components. \gls{pca} is among the simple and most straightforward ways of doing dimensionality reduction. It is a technique that reduces high-dimensional data into a lower-dimensional representation by using the dependencies between variables without losing too much information \cite{Shalizi2012}. After the features are reduced using \gls{pca}, then on these reduced features, Gaussian \gls{kde} is applied. The main idea behind \gls{kde} is that the training datasets will follow some random distribution, and this distribution can be modeled by employing \gls{kde} \cite{10208786}.

During inference, if a lower probability density is observed below the threshold determined by the training dataset, this indicates the existence of an anomaly compared to the data distribution learned from the training data \cite{10208786}. The \gls{dfkde} gives competitive results on MVTec AD\cite{8954181}, for the metric \gls{auroc} \gls{dfkde} gives the highest of 96.5 score \cite{Anomalib2024}.

\subsection{FastFlow}
\label{subsec:fastflow}

Most existing representation-based methods use a deep convolutional neural network to extract normal image features and then characterize this distribution through non-parametric distribution estimation methods. However, these methods fail to map features effectively to a tractable base distribution and ignore the relationship between the local and global features, which are essential for anomaly detection. Therefore, FastFlow is proposed to mitigate these problems. FastFlow is an unsupervised anomaly detection and localization model which is built with 2D normalization flows as its probability distribution estimator. Experimental findings show that FastFlow outperforms previous state-of-the-art methods in terms of both accuracy and inference efficiency. It achieves 99.4\% AUC in anomaly detection \cite{yu2021fastflowunsupervisedanomalydetection}.

\subsubsection*{Core Concept :}

Earlier unsupervised approaches used non-parametric methods, while recent ones started using normalization flow to model the distribution of features for normal images. However, these one-dimensional normalization flow models require flattening of the two-dimensional input feature into a one-dimensional vector to estimate the distribution, which destroys the positional relationship of the 2D image and limits the ability of the flow model. These models also used a sliding window approach to extract features from a large number of image patches and detect anomalies for each patch. This led to high inference complexity. Therefore, to address the above problems, FastFlow extends the normalizing flow to two-dimensional space by using fully connected neural networks as the subnet, which can maintain the relative position of the space and support end-to-end inference of the whole image. This improves the anomaly detection performances and gives the detection and localization results at once for the whole image to improve inference efficiency \cite{yu2021fastflowunsupervisedanomalydetection}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.1\linewidth]{Rohit_Master_Thesis//Images/fastflow_pipeline.png}
    \caption{FastFlow pipeline\cite{yu2021fastflowunsupervisedanomalydetection}}
    \label{fig:fastflow pipeline}
\end{figure}

Figure \ref{fig:fastflow pipeline} shows the FastFlow pipeline, in which first, the visual features are extracted using the feature extractor and then passed as an input to the FastFlow module for probability density estimation. During training, FastFlow is trained using normal images to convert the normal distribution into a standard normal distribution in a 2D manner. For inferencing, an anomaly score is assigned to each location on the two-dimensional feature based on its probability values \cite{yu2021fastflowunsupervisedanomalydetection}.

The FastFlow pipeline consists of two main components, they are:

1. Feature Extractor,

2. 2D Normalization Flow

\subsubsection*{Feature Extractor :}

The first step in the whole pipeline involves extracting representative features from the input image using either ResNet as explained in section \ref{subsec:ResNet} or \gls{vit}. In \glspl{vit} as a feature extractor, features from only one layer are extracted because of their capability to capture the relationship between local patches and global features. In the case of ResNet, features are taken directly from the last layer in the first three blocks, and then these features are inputted into three corresponding FastFlow models \cite{yu2021fastflowunsupervisedanomalydetection}. 

\subsubsection*{2D Normalization Flow :}

The 2D flow function is used to project the image features into the hidden variable using a bijective invertible mapping. At the time of inference, the features of anomalous images should be out of distribution and, therefore, will have lower likelihoods than normal images. This likelihood can be used as the anomaly score. Specifically, the sum of the 2D probabilities of each channel is done to obtain the final probability map, and then it is upsampled to the input image resolution using bilinear interpolation. In order to convert the original normalization flow into a 2D format, alternate $3\times3$ and $1\times1$ convolutional layers are used in the default subnet as shown in the figure\ref{fig:fastflow pipeline} to retain the spatial information in the flow model and the loIn the feature extraction stage, the features are extracted using \gls{dnn} as the backbone, such as ResNet, which was pre-trained on ImageNet\cite{5206848} dataset \cite{10208786}., CIFAR-10\cite{krizhevsky2009learning}, BTAD\cite{Mishra_2021} datasets. For the MVTec AD dataset, the FastFlow model was compared with many state-of-the-art models with two metrics image-level \gls{auc} and pixel-level \gls{auc}. FastFlow demonstrates exceptional performance in anomaly detection, achieving a 99.4 image-level \gls{auc} and 98.5 pixel-level \gls{auc}, surpassing all the other models. In the case of the CIFAR-10 dataset, FastFlow achieves an \gls{auc} 66.7, which is the best performing model when compared with others. For the BTAD dataset, FastFlow again surpasses all the compared models and achieves a pixel-level \gls{auc} of 97.0 \cite{yu2021fastflowunsupervisedanomalydetection}.

\subsection{EfficientAD}
\label{subsec:efficientad}

EfficientAD is an effective anomaly detection model for real-world computer vision applications focusing on computational efficiency and a lightweight feature extractor that can process an image under a millisecond using a modern GPU. EfficientAD employs a \gls{s-t} approach to detect anomalous features. It establishes new standards for detecting and localizing the anomalies, coupled with its low error rate. This makes EffienctAD an economical option for real-world applications \cite{batzner2024efficientadaccuratevisualanomaly}.

\subsubsection*{Core Concept :}

EffientAD sets new benchmarks for both the accuracy of anomaly detection and the speed of inference. In order to detect anomalous features, a \gls{s-t} approach is used, where the student network is trained to predict features computed by a pre-trained teacher network on normal training images. EffienctAD introduces training loss that prevents the student network from imitating the teacher network beyond the normal images. This loss enables the use of efficient network architecture for both the student and the teacher while improving anomaly detection \cite{batzner2024efficientadaccuratevisualanomaly}. The components of EfficientAD can be divided into three main components:

1. Efficient \gls{pdn},

2. Lightweight Student-Teacher model,

3. Autoencoder for Logical Anomaly Detection,

\subsubsection*{Efficient Patch Description Network (PDN) :}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{Rohit_Master_Thesis//Images/pdn.png}
    \caption{\gls{pdn} architecture for EfficientAD. All features can be obtained in a single forward pass using it on an image in a fully convolutional manner \cite{batzner2024efficientadaccuratevisualanomaly}.}
    \label{fig:pdn architecture}
\end{figure}

Recent anomaly detection methods often employ deep pre-trained networks for feature extraction. EffienctAD employs a feature extractor network of drastically reduced depth. This feature extractor is referred to as \gls{pdn}, and its architecture, as shown in figure \ref{fig:pdn architecture}, contains only four convolutional layers. The receptive field of each output neuron is $33\times33$ pixels, which makes \gls{pdn} possible to generate all feature vectors in a single forward pass for images of different sizes \cite{batzner2024efficientadaccuratevisualanomaly}.

To overcome the large feature map size, high computational costs, and memory requirement problems, \gls{pdn} performs early stage downsampling by strided average-pooling layers after the first and second convolutional layers as shown in figure \ref{fig:pdn architecture}. To generate expressive features, \gls{pdn} is trained by distilling a deep pre-trained classification network into it. \gls{pdn} trained in images from ImageNet by reducing the mean squared difference between its output and the features extracted from the pre-trained network. Besides higher efficiency, this has another benefit in that the features generated by \gls{pdn} only depend on local pixels, avoiding long-range dependencies in other methods. This ensures that an anomaly detected in one area of the image cannot activate anomalous feature vectors in other distant areas, which can compromise the localization of anomalies \cite{batzner2024efficientadaccuratevisualanomaly}.

\subsubsection*{Lightweight Student-Teacher model :}

For anomalous feature vector detection, EfficientAD uses a \gls{s-t} approach, with the teacher being the \gls{pdn}. The student network also uses the teacher's architecture to maintain low overall latency. The EfficientAD model introduces a training loss, substantially improving anomaly detection without affecting the computational test time requirements. The goal is to provide the student with enough data so that it can mimic the teacher on normal images while avoiding generalization on anomalous images. Therefore, the student's loss is restricted to the most relevant parts of an image, i.e., the patches where the student mimics the teacher the least. A hard feature loss is proposed, which primarily uses the output elements with the highest loss for the purpose of backpropagation \cite{batzner2024efficientadaccuratevisualanomaly}.

Apart from hard feature loss, EfficientAD adds a loss penalty during training that further hinders the student's ability to imitate the teacher on images that are not included in the normal training images. The teacher network is either pre-trained on an image classification dataset or is a distilled version of a pre-trained network. However, the student is only trained on the application's normal images. To hinder the student's ability to generalize its imitation of the teacher to out-of-distribution images, a penalty in terms of using images from the teacher's pretraining during the training of student is proposed \cite{batzner2024efficientadaccuratevisualanomaly}.

\subsubsection*{Autoencoder for Logical Anomaly Detection :}

For learning logical anomalies of the training images and detecting any violations of these constraints, EfficientAD uses an autoencoder on training images. Figure \ref{fig:EfficientAD pipeline} illustrates the anomaly detection pipeline for EfficientAD. It consists of \gls{s-t} pair and an autoencoder. The autoencoder is trained to predict the output generated by teacher's output. A standard convolutional autoencoder is used, which consists of strided convolutions in the encoder and bilinear upsampling in the decoder \cite{batzner2024efficientadaccuratevisualanomaly}.

Usually, for both the logical anomalies as well as normal images, the autoencoder fails to generate the accurate latent code needed for reconstruction of the image in the teacher's feature space, as can be seen in the figure \ref{fig:EfficientAD pipeline}. Using the difference between the teacher's output and the autoencoder's reconstruction as an anomaly map might cause false-positive detections. To avoid this, the student's output channels are doubled and trained to predict the output of the autoencoder in addition to the output of the teacher \cite{batzner2024efficientadaccuratevisualanomaly}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.1\linewidth]{Rohit_Master_Thesis/Images/efficientAD_pipeline.pdf}
    \caption{EfficientAD pipeline}
    \label{fig:EfficientAD pipeline}
\end{figure}

The student learns the systematic reconstruction errors of the autoencoder on normal images. However, it does not learn the reconstruction errors for anomalies as they are not part of the training set. This makes the difference between the student's and the autoencoder's output suitable for computing the anomaly map, which is calculated as the squared difference between the two outputs and then averaged across all channels. EfficientAD produces two anomaly maps, the one generated by the autoencoder-student pair is called a global anomaly map, and the one generated by \gls{s-t} pair is called a local anomaly map as seen in the figure \ref{fig:EfficientAD pipeline}. These two maps are then normalized to a similar scale and averaged to generate the combined anomaly map. Its maximum value is used as the image-level anomaly score \cite{batzner2024efficientadaccuratevisualanomaly}.

\subsubsection*{Performance :}

EfficientAD achieves an impressive image-level \gls{auroc} score of $99.8\%$ on MVTec AD dataset\cite{8954181} with early stopping enabled. For the overall anomaly detection performance, EfficientAD achieves very strong image-level detection and pixel-level localization performance with the highest score of $98.1$ for VisA\cite{zou2022spotthedifferenceselfsupervisedpretraininganomaly} dataset. The computational cost of EfficientAD was measured using the metrics latency and throughput, with the EfficientAD showing latency of $2.2ms$ and throughput of 614 $img/s$ \cite{batzner2024efficientadaccuratevisualanomaly}.

\subsection*{Summary of all the models}

The table \ref{tab:model_key_characteristics} summarizes all the models we have gone through above.

\begin{table}[ht!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|p{0.7\textwidth}|}
\hline
\textbf{Model} & \textbf{Key Characteristics} \\ \hline
YOLO & Real-time object detection; maps pixels to bounding box coordinates and class probabilities \\ \hline
PatchCore & Uses locally-aware patch features and memory banks for anomaly detection \\ \hline
DFM (Deep Feature Modelling) & Models normal data distribution using deep feature extraction and density modeling \\ \hline
EfficientAD & Combines lightweight student-teacher networks with efficient patch descriptors \\ \hline
FastFlow & Extends normalizing flow models to 2D for feature distribution estimation \\ \hline
DFKDE (Deep Feature Kernel Density Estimation) & Extracts features with \gls{dnn} and applies Kernel Density Estimation on reduced features \\ \hline
\end{tabular}%
}
\caption{Summary of Models and their Key Characteristics}
\label{tab:model_key_characteristics}
\end{table}

